{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from unicodedata import normalize\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import transformers\n",
    "import textclassification as tc\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    if type(x) is str:\n",
    "        pattern = r'[^a-zA-z0-9!.,?\\s]'\n",
    "        x = normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII')\n",
    "        x = re.sub(pattern, '', x)\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing texts...\n"
     ]
    }
   ],
   "source": [
    "df            = pd.read_csv(\"../../data/marcapasso/train.csv\")\n",
    "dicionarioCsv = pd.read_csv(\"../../data/marcapasso/dicionario.csv\")\n",
    "\n",
    "print(\"Preparing texts...\")\n",
    "texts = [\" \" + tc.clean_text(text) + \" \" for text in df[\"texto\"]]\n",
    "    \n",
    "dicionario = {}\n",
    "for row in dicionarioCsv.itertuples():\n",
    "    aux = []\n",
    "    for diag in row:\n",
    "        if type(diag) is str: aux.append(tc.clean_text(diag))\n",
    "    dicionario[row[0]] = aux\n",
    "    if row[0] == 9: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating scores...\n",
      "\n",
      "Working in batches of 500\n",
      "0 / 10483\n",
      "This batch has been done in 0 minutes and 9.857285737991333 seconds!\n",
      "Expected time for ending is around 0 hours and  3 minutes!\n",
      "500 / 10483\n",
      "This batch has been done in 0 minutes and 8.01957893371582 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "1000 / 10483\n",
      "This batch has been done in 0 minutes and 11.06112289428711 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "1500 / 10483\n",
      "This batch has been done in 0 minutes and 11.172068357467651 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "2000 / 10483\n",
      "This batch has been done in 0 minutes and 10.840376138687134 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "2500 / 10483\n",
      "This batch has been done in 0 minutes and 9.46355676651001 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "3000 / 10483\n",
      "This batch has been done in 0 minutes and 12.912248134613037 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "3500 / 10483\n",
      "This batch has been done in 0 minutes and 10.65104341506958 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "4000 / 10483\n",
      "This batch has been done in 0 minutes and 8.967167139053345 seconds!\n",
      "Expected time for ending is around 0 hours and  2 minutes!\n",
      "4500 / 10483\n",
      "This batch has been done in 0 minutes and 9.636683940887451 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "5000 / 10483\n",
      "This batch has been done in 0 minutes and 9.244696855545044 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "5500 / 10483\n",
      "This batch has been done in 0 minutes and 11.77948808670044 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "6000 / 10483\n",
      "This batch has been done in 0 minutes and 15.2349693775177 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "6500 / 10483\n",
      "This batch has been done in 0 minutes and 13.324326515197754 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "7000 / 10483\n",
      "This batch has been done in 0 minutes and 11.43553638458252 seconds!\n",
      "Expected time for ending is around 0 hours and  1 minutes!\n",
      "7500 / 10483\n",
      "This batch has been done in 0 minutes and 12.874417781829834 seconds!\n",
      "Expected time for ending is around 0 hours and  0 minutes!\n",
      "8000 / 10483\n",
      "This batch has been done in 0 minutes and 11.855341672897339 seconds!\n",
      "Expected time for ending is around 0 hours and  0 minutes!\n",
      "8500 / 10483\n",
      "This batch has been done in 0 minutes and 11.196832180023193 seconds!\n",
      "Expected time for ending is around 0 hours and  0 minutes!\n",
      "9000 / 10483\n",
      "This batch has been done in 0 minutes and 10.637771129608154 seconds!\n",
      "Expected time for ending is around 0 hours and  0 minutes!\n",
      "9500 / 10483\n",
      "This batch has been done in 0 minutes and 10.180964946746826 seconds!\n",
      "Expected time for ending is around 0 hours and  0 minutes!\n",
      "10000 / 10483\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-01efaa02a5e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#     except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Calculating scores...\\n\")\n",
    "\n",
    "\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "def return_scores(text, dicionario):\n",
    "    scores = [max([tc.make_score(text, diag) for diag in dicionario[i]]) for i in range(10)]\n",
    "    return scores\n",
    "\n",
    "batch = 500\n",
    "print(\"Working in batches of\", batch)\n",
    "with open('../processed_data/scores_marcapasso_tf.csv', 'w') as f:\n",
    "    f.write(\"id, scorings\\n\")\n",
    "    #     errors = []\n",
    "\n",
    "    \n",
    "startTime = time.time()\n",
    "for i in range(0, len(texts), batch):\n",
    "    print(i,\"/\",len(texts))\n",
    "    startBatch = time.time()\n",
    "    #try:\n",
    "#     scores = [return_scores(text, dicionario) for text in texts[i:i+batch]]\n",
    "    scores = Parallel(n_jobs = N_CORES)(delayed(return_scores)\n",
    "                              (text, dicionario)\n",
    "                    for text in texts[i:i+batch])\n",
    "    \n",
    "    if(i == 0): result = np.array(scores)\n",
    "    else: result = np.concatenate((result, np.array(scores)), axis = 0)\n",
    "    with open('../processed_data/scores_marcapasso_tf.csv', 'a') as f:\n",
    "        for j in range(i,i+batch):\n",
    "            f.write(str(j))\n",
    "            f.write(',\"')\n",
    "            f.write(str(scores[j-i]))\n",
    "            f.write('\"\\n')\n",
    "#     except:\n",
    "#         print(\"ERROR!!!!!\")\n",
    "#         errors.append([i, i+batch])\n",
    "#     errors = np.array(errors)\n",
    "#     np.save(\"errors.npy\", errors)\n",
    "    \n",
    "    expectedTime = (((time.time() - startTime)/(i+batch)) * (len(texts))) - (time.time() - startTime)\n",
    "    timeBatch    = time.time() - startBatch\n",
    "    print(\"This batch has been done in\", int(timeBatch/60), \"minutes and\", timeBatch%60,\"seconds!\")\n",
    "    print(\"Expected time for ending is around\", int(expectedTime/3600), \"hours and \", int((expectedTime%3600)/60),\"minutes!\")\n",
    "#         with open('../../data/resultados/scorings1.csv', 'a') as f:\n",
    "#             for j in range(i,i+batch):\n",
    "#                 f.write(str(db[\"ID_EXAME\"][j]))\n",
    "#                 f.write(',\"')\n",
    "#                 f.write(str(scores[j-i]))\n",
    "#                 f.write('\"\\n')\n",
    "print(\"Y of training data defined!!! Saving...\")\n",
    "np.save(\"results/score_marcapasso.npy\", result)\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.load(\"results/score_marcapasso.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_res = np.array([[1 if i >= 70 else 0 for i in row] for row in result])\n",
    "np.save(\"results/mp_bin_scores.npy\", bin_res)\n",
    "y_train = bin_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(\"results/mp_bin_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807167 Training sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # Only consider the top 20k words\n",
    "maxlen = 411\n",
    "x_train = np.load(\"../data/x_train.npy\", allow_pickle = True)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "# print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 411\n",
    "with open(\"../main/output/25_Jul_2020/tokenizer_10000.pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "x_train = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "for row in x_train:\n",
    "    while(len(row) < 366):\n",
    "        row.append(0)\n",
    "        \n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6776683403940868913\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "out_dim = 10\n",
    "maxlen = 366\n",
    "vocab_size = 10000\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block3 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block4 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block5 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block6 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block1(x)\n",
    "x = transformer_block2(x)\n",
    "x = transformer_block3(x)\n",
    "x = transformer_block4(x)\n",
    "x = transformer_block5(x)\n",
    "x = transformer_block6(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(out_dim, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mp_train/x_train.npy\", x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay let's train\n",
      "Epoch 1/10\n",
      " 95/295 [========>.....................] - ETA: 14:43 - loss: 0.0487 - accuracy: 0.7431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c9d288bbb764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Okay let's train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', min_delta=1),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Okay, training\n",
    "print(\"Okay let's train\")\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=10, validation_split=0.1,\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', min_delta=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='best_model_mp', monitor='val_accuracy', save_best_only=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/grad/ccomp/19/joao.pedrosa/miniconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/grad/ccomp/19/joao.pedrosa/miniconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: last_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "maxlen = 411\n",
    "with open(\"../main/output/25_Jul_2020/tokenizer_\"+str(vocab_size)+\".pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing texts...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing texts...\")\n",
    "text        = [clean_text(t) for t in text]\n",
    "test_X_temp = tokenizer.texts_to_sequences(text)\n",
    "test_X      = keras.preprocessing.sequence.pad_sequences(test_X_temp, maxlen=maxlen)\n",
    "test_Y      = np.load(\"../light_data/gold_labels.npy\", allow_pickle = True)\n",
    "valid       = np.load(\"../light_data/new_classes.npy\")[:,1]\n",
    "# test_Y = test_Y[:,0]\n",
    "y_test      = np.array(np.array([[int(row[i]) for i in range(len(row)) if valid[i]] for row in test_Y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc3404a5b50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('last_model')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real1</th>\n",
       "      <th>real2</th>\n",
       "      <th>real3</th>\n",
       "      <th>real4</th>\n",
       "      <th>real5</th>\n",
       "      <th>real6</th>\n",
       "      <th>real7</th>\n",
       "      <th>real8</th>\n",
       "      <th>real9</th>\n",
       "      <th>...</th>\n",
       "      <th>regex1</th>\n",
       "      <th>regex2</th>\n",
       "      <th>regex3</th>\n",
       "      <th>regex4</th>\n",
       "      <th>regex5</th>\n",
       "      <th>regex6</th>\n",
       "      <th>regex7</th>\n",
       "      <th>regex8</th>\n",
       "      <th>regex9</th>\n",
       "      <th>regex10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  real1  real2  real3  real4  real5  real6  real7  real8  real9  ...  \\\n",
       "0      0      1      0      0      0      0      0      0      0      0  ...   \n",
       "1      1      1      0      0      0      0      0      0      0      0  ...   \n",
       "2      2      1      0      0      0      0      0      0      0      0  ...   \n",
       "3      3      1      0      0      0      0      0      0      0      0  ...   \n",
       "4      4      1      0      0      0      0      0      0      0      0  ...   \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "131  131      0      0      0      0      0      0      0      0      0  ...   \n",
       "132  132      0      0      0      0      0      0      0      0      0  ...   \n",
       "133  133      0      0      0      0      0      0      0      0      0  ...   \n",
       "134  134      0      0      0      0      0      0      0      0      0  ...   \n",
       "135  135      0      0      0      0      0      0      1      0      0  ...   \n",
       "\n",
       "     regex1  regex2  regex3  regex4  regex5  regex6  regex7  regex8  regex9  \\\n",
       "0         1       0       0       0       0       0       0       0       0   \n",
       "1         1       0       0       0       0       0       0       0       0   \n",
       "2         1       0       0       0       0       0       0       0       0   \n",
       "3         1       0       0       0       0       0       0       0       0   \n",
       "4         1       0       0       0       0       0       0       0       0   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "131       0       0       0       0       0       0       0       0       0   \n",
       "132       0       0       0       0       0       0       0       0       0   \n",
       "133       0       0       0       0       0       0       0       0       0   \n",
       "134       0       0       0       0       0       0       0       0       0   \n",
       "135       0       0       0       0       0       0       1       0       0   \n",
       "\n",
       "     regex10  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "131        0  \n",
       "132        0  \n",
       "133        0  \n",
       "134        0  \n",
       "135        0  \n",
       "\n",
       "[136 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"../light_data/gold_mp.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels    = np.array(pd.read_csv(\"../light_data/gold_mp.csv\", sep = ';'))[:,1:11]\n",
    "test_text = pd.read_csv(\"../../data/marcapasso/texto.csv\", sep = ';')[\"text\"]\n",
    "test_text = [tc.clean_text(t) for t in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "for row in test_text:\n",
    "    while(len(row) < 366):\n",
    "        row.append(0)\n",
    "\n",
    "x_test = np.array(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mp_train/x_test.npy\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict(x_test, batch_size = 32)\n",
    "np.save(\"predict_tf_best_mp.npy\", y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mp_train/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating best candidates for threshold...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating best candidates for threshold...\")\n",
    "n_class = 10\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "thresholds = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresholds[i] = metrics.roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/pedrosa/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 10%|█         | 1/10 [00:00<00:01,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 17.64it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating thresholds...\")\n",
    "#Calculate limits by maximizing F1\n",
    "limits = []\n",
    "for j in tqdm(range(n_class)):\n",
    "    bigf1 = 0\n",
    "    for threshold in thresholds[j]:\n",
    "        y_bin = []\n",
    "        for row in y_score[:,j]:\n",
    "            if row > threshold:\n",
    "                y_bin.append(1)\n",
    "            else:\n",
    "                y_bin.append(0)\n",
    "        y_bin = np.array(y_bin)\n",
    "        precision, _, f1, _ = precision_recall_fscore_support(y_test[:,j], y_bin, average = 'binary')\n",
    "        \n",
    "        if(f1 > bigf1 and precision > 0):\n",
    "            bigf1 = f1\n",
    "            maxi = threshold\n",
    "    limits.append(maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_classes_antigo = ['área_eletricamente_inativa',\n",
    "       'Bloqueio_de_ramo_direito', 'Bloqueio_de_ramo_esquerdo',\n",
    "       'Bloqueio_de_ramo_direito_e_bloqueio_divisional_anterossuperior_do_ramo_esquerdo',\n",
    "       'Bloqueio_intraventricular_inespecífico',\n",
    "       'Sobrecarga_ventricular_esquerda_(critérios_de_Romhilt-Estes)',\n",
    "       'Sobrecarga_ventricular_esquerda_(critérios_de_voltagem)',\n",
    "       'Fibrilação_atrial', 'Flutter_atrial',\n",
    "       'Bloqueio_atrioventricular_de_2°_grau_Mobitz_I',\n",
    "       'Bloqueio_atrioventricular_de_2°_grau_Mobitz_II',\n",
    "       'Bloqueio_atrioventricular_2:1', 'Bloqueio_atrioventricular_avançado',\n",
    "       'Bloqueio_atrioventricular_total',\n",
    "       'Pré-excitação_ventricular_tipo_Wolff-Parkinson-White',\n",
    "       'Sistema_de_estimulação_cardíaca_normofuncionante',\n",
    "       'Sistema_de_estimulação_cardíaca_com_disfunção',\n",
    "       'Taquicardia_atrial_multifocal', 'Taquicardia_atrial',\n",
    "       'Taquicardia_supraventricular', 'Corrente_de_lesão_subendocárdica',\n",
    "       'Alterações_primárias_da_repolarização_ventricular',\n",
    "       'Extrassístoles_supraventriculares', 'Extrassístoles_ventriculares',\n",
    "       'Bradicardia_sinusal',\n",
    "       'ECG_dentro_dos_limites_da_normalidade_para_idade_e_sexo',\n",
    "       'Alterações_da_repolarização_ventricular_atribuídas_à_ação_digitálica',\n",
    "       'Alterações_inespecíficas_da_repolarização_ventricular',\n",
    "       'Alterações_secundárias_da_repolarização_ventricular',\n",
    "       'Arritmia_sinusal',\n",
    "       'Ausência_de_sinal_eletrocardiográfico_que_impede_a_análise',\n",
    "       'Interferência_na_linha_de_base_que_não_impede_a_análise_do_ECG',\n",
    "       'Ausência_de_sinal_eletrocardiográfico_que_não_impede_a_análise',\n",
    "       'Traçado_com_qualidade_técnica_insuficiente',\n",
    "       'Possível_inversão_de_posicionamento_de_eletrodos',\n",
    "       'Baixa_voltagem_em_derivações_precordiais',\n",
    "       'Baixa_voltagem_em_derivações_periféricas',\n",
    "       'Bloqueio_atrioventricular_de_1°_grau',\n",
    "       'Bloqueio_de_ramo_direito_e_bloqueio_divisional_posteroinferior_do_ramo_esquerdo',\n",
    "       'Bloqueio_divisional_anterossuperior_do_ramo_esquerdo',\n",
    "       'Bloqueio_divisional_posteroinferior_do_ramo_esquerdo',\n",
    "       'Desvio_do_eixo_do_QRS_para_direita',\n",
    "       'Desvio_do_eixo_do_QRS_para_esquerda',\n",
    "       'Dissociação_atrioventricular_isorrítmica',\n",
    "       'Distúrbio_de_condução_do_ramo_direito',\n",
    "       'Distúrbio_de_condução_do_ramo_esquerdo', 'Intervalo_PR_curto',\n",
    "       'Intervalo_QT_prolongado', 'Isquemia_subendocárdica',\n",
    "       'Progressão_lenta_de_R_nas_derivações_precordiais', 'Pausa_sinusal',\n",
    "       'Corrente_de_lesão_subepicárdica',\n",
    "       'Corrente_de_lesão_subepicárdica_-_provável_infarto_agudo_do_miocárdio_com_supradesnivelamento_de_ST',\n",
    "       'Repolarização_precoce', 'Ritmo_atrial_ectópico',\n",
    "       'Ritmo_atrial_multifocal', 'Ritmo_idioventricular_acelerado',\n",
    "       'Ritmo_juncional', 'Síndrome_de_Brugada', 'Sobrecarga_atrial_direita',\n",
    "       'Sobrecarga_atrial_esquerda', 'Sobrecarga_biatrial',\n",
    "       'Sobrecarga_biventricular', 'Sobrecarga_ventricular_direita',\n",
    "       'Sobrecarga_ventricular_esquerda(_critérios_de_voltagem)',\n",
    "       'Taquicardia_sinusal', 'Taquicardia_ventricular_não_sustentada',\n",
    "       'Taquicardia_ventricular_sustentada',\n",
    "       'Suspeita_de_Síndrome_de_Brugada_repetir_V1-V2_em_derivações_superiores',\n",
    "       'Taquicardia_juncional', 'Batimento_de_escape_atrial',\n",
    "       'Batimento_de_escape_supraventricular', 'Batimento_de_escape_juncional',\n",
    "       'Batimento_de_escape_ventricular']\n",
    "\n",
    "ordem_classes = ['área_eletricamente_inativa',\n",
    " 'Bloqueio_de_ramo_direito',\n",
    " 'Bloqueio_de_ramo_esquerdo',\n",
    " 'Sobrecarga_ventricular_esquerda_(critérios_de_Romhilt-Estes)',\n",
    " 'Fibrilação_atrial',\n",
    " 'Flutter_atrial',\n",
    " 'Bloqueio_atrioventricular_de_2°_grau_Mobitz_I',\n",
    " 'Pré-excitação_ventricular_tipo_Wolff-Parkinson-White',\n",
    " 'Sistema_de_estimulação_cardíaca_normofuncionante',\n",
    " 'Taquicardia_atrial_multifocal',\n",
    " 'Taquicardia_supraventricular',\n",
    " 'Alterações_primárias_da_repolarização_ventricular',\n",
    " 'Extrassístoles_supraventriculares',\n",
    " 'Extrassístoles_ventriculares',\n",
    " 'Bradicardia_sinusal',\n",
    " 'ECG_dentro_dos_limites_da_normalidade_para_idade_e_sexo',\n",
    " 'Alterações_inespecíficas_da_repolarização_ventricular',\n",
    " 'Alterações_secundárias_da_repolarização_ventricular',\n",
    " 'Arritmia_sinusal',\n",
    " 'Ausência_de_sinal_eletrocardiográfico_que_impede_a_análise',\n",
    " 'Possível_inversão_de_posicionamento_de_eletrodos',\n",
    " 'Bloqueio_atrioventricular_de_1°_grau',\n",
    " 'Bloqueio_divisional_anterossuperior_do_ramo_esquerdo',\n",
    " 'Bloqueio_divisional_posteroinferior_do_ramo_esquerdo',\n",
    " 'Desvio_do_eixo_do_QRS_para_direita',\n",
    " 'Desvio_do_eixo_do_QRS_para_esquerda',\n",
    " 'Distúrbio_de_condução_do_ramo_direito',\n",
    " 'Distúrbio_de_condução_do_ramo_esquerdo',\n",
    " 'Intervalo_PR_curto',\n",
    " 'Intervalo_QT_prolongado',\n",
    " 'Isquemia_subendocárdica',\n",
    " 'Progressão_lenta_de_R_nas_derivações_precordiais',\n",
    " 'Ritmo_atrial_ectópico',\n",
    " 'Sobrecarga_atrial_esquerda',\n",
    " 'Taquicardia_sinusal']\n",
    "ordem_classes_eng = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordem_classes = [\"Chagas Disease\",\n",
    "\"Schemic Cardiomyopathy\",\n",
    "\"Valvular Heart Disease\",\n",
    "\"Hypertrophic Cardiomyopathy\",\n",
    "\"Congenic Cardiopatics\",\n",
    "\"Long QT Syndrome\",\n",
    "\"Brugada Syndrome\",\n",
    "\"Idiopathic Ventricular Fibrillation\",\n",
    "\"Arrhythmogenic Dysplasia of VD\", \n",
    "\"Idiopathic Cardiomyopathy\"]\n",
    "ordem_classes = np.array(pd.read_csv(\"../../data/marcapasso/dicionario.csv\"))[:,1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chagas', 'miocardiopatia isquêmica', 'cardiopatia valvar',\n",
       "       'cardiomiopatia hipertrófica', 'cardiopatia congênita',\n",
       "       'síndrome do QT longo', 'síndrome de Brugada',\n",
       "       'fibrilação ventricular idiopática',\n",
       "       'displasia arritmogênica do VD', 'miocardiopatia idiopática'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordem_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "date = today.strftime(\"%d_%b_%Y\")\n",
    "import os, sys\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.exists(\"results/\"+date):\n",
    "    os.mkdir(\"results/\"+date)\n",
    "# if not os.path.exists(\"results/\"+date+\"/ROC\"):\n",
    "#     os.mkdir(\"results/\"+date+\"/ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating results...\")\n",
    "#Function to turn score in binary\n",
    "#Here the thresholds must be a list of 10 numbers\n",
    "def getMetrics(y_test, y_score, thresholds):\n",
    "    \n",
    "    #First we turn into binary\n",
    "    y_bin = []\n",
    "    for j in range(len(y_score)):\n",
    "        ans = []\n",
    "        for i in range(n_class):\n",
    "            if y_score[j][i] >= thresholds[i]:\n",
    "#             if y_label[j][i]:\n",
    "                ans.append(1)\n",
    "            else:\n",
    "                ans.append(0)\n",
    "        y_bin.append(np.array(ans))\n",
    "    y_bin = np.array(y_bin)\n",
    "    np.save(\"bin_tf_last.npy\", y_bin)\n",
    "    \n",
    "    #Then we calculate\n",
    "    target_names = [\"(\" + ordem_classes[i] + \") Class\" + str(i) for i in range(n_class)]\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    f1 = dict()\n",
    "    sup = dict()\n",
    "    for i in range(n_class):\n",
    "        precision[i], recall[i], f1[i], sup[i] = precision_recall_fscore_support(y_test[:,i], y_bin[:,i], average = 'binary')\n",
    "    return precision, recall, f1, sup\n",
    "\n",
    "#z = 0.1\n",
    "#while(z < 1):\n",
    "precision, recall, f1, _ = getMetrics(y_test, y_score, limits)\n",
    "#   z += 0.1\n",
    "f1 = f1.items()\n",
    "df = pd.DataFrame(columns = [\"Class\", \"Precision\", \"Recall\", \"F1\", \"Ocurrences\"])\n",
    "for row in f1:\n",
    "    n = row[0]\n",
    "    sup = y_test[:,n].sum()\n",
    "    new_row = {'Class': str(ordem_classes[n]), 'Precision': precision[n], 'Recall': recall[n], 'F1': row[1], \"Ocurrences\": sup}\n",
    "    df = df.append(new_row, ignore_index = True)\n",
    "df = df.set_index(\"Class\")\n",
    "df.to_csv(\"results/\"+date+\"/resultLastModelMP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.load(\"bin_tf.npy\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1455 - accuracy: 0.4535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14547136425971985, 0.4534973204135895]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
