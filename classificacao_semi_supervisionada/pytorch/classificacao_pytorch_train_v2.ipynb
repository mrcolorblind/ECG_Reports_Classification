{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from unicodedata import normalize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "REBUILD_DATA = False\n",
    "BIDIRECTIONAL = True\n",
    "TOP_WORDS = 10000\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 256\n",
    "#Essa variável aqui é o número de posições que os vetores tem ao usar o pad_sequences. Não uso ela pra nada ainda mas talvez eu venha a usar, por isso ela está aqui.\n",
    "# SEQ_SIZE = 390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a GPU :D\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on a GPU :D\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on a CPU :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oi, Derick\n",
    "# To comentando como diálogo porque acho mais fácil explicar o código assim. Se o comentário estiver errado é pq essa é a parte que eu entendi errado.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, seq_size):\n",
    "        \n",
    "        print(\"Building NN...\")\n",
    "        embedding_dim = 128\n",
    "        lstm_out_dim = 128\n",
    "        num_embeddings = TOP_WORDS\n",
    "        num_of_classes = 35\n",
    "        \n",
    "        super().__init__()\n",
    "        #Camada de Embedding, o padding_idx é um argumento que eu descobri que é usada para falar para a camada que os números no fim de cada vetor são apenas lixo\n",
    "        self.l1 = nn.Embedding(num_embeddings, embedding_dim, padding_idx = 0)\n",
    "        #Eu não entendo muito bem o que essa camada faz. Pelo que eu entendi é algo probabilístico. Mas ela n altera o shape.\n",
    "#         self.l2 = nn.Dropout(p=0.4)\n",
    "        #A LSTM recebe os Embeddings e cospe o mesmo número de vetores que eu passei para ela. Não sei se eu deveria alterar o número de camadas da LSTM.\n",
    "        #Se usar menos de 2 não dá pra colocar Dropout pq o Dropout é aplicado em todas as camadas menos na última.\n",
    "        self.l3 = nn.LSTM(embedding_dim, lstm_out_dim, dropout = 0.2, num_layers = 2, bidirectional = BIDIRECTIONAL)\n",
    "        #É o seguinte. Como as dimensões de entrada são estáticas, eu adicionei elas manualmente na camada linear para conseguir fazer o flatten.\n",
    "        self.l4 = nn.Flatten()\n",
    "        #Dimensao do vetor de entrada X dimensao da lstm\n",
    "        self.l5 = nn.Linear(seq_size * lstm_out_dim * (2 if BIDIRECTIONAL else 1), num_of_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Aqui eu só to passando o input pelas camadas mesmo\n",
    "        x    = self.l1(x)\n",
    "#         x    = self.l2(x)\n",
    "        #A camada de LSTM retorna uma tupla, o vetor que eu quero é a primeira posição da tupla, por isso recebo assim.\n",
    "        #Acho que a segunda camada da LSTM só é util ao passar de uma camada da LSTM para a outra.\n",
    "        x, _ = self.l3(x)\n",
    "        x    = self.l4(x)\n",
    "        x    = self.l5(x)\n",
    "        #Aqui eu aplico o softmax. Especifico o número de dimensões para ser um e tal. Não sei o que não está funcionando :c.\n",
    "        x    = F.softmax(x, dim = 1)\n",
    "            \n",
    "        return x             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    if type(x) is str:\n",
    "        pattern = r'[^a-zA-z0-9!:.,?\\s]'\n",
    "        x = normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII')\n",
    "        x = re.sub(pattern, '', x)\n",
    "        return x\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading data:\\n\")\n",
    "    print(\"Ids...\")\n",
    "    greenZoneIdx = np.load('../../light_data/greenZoneIndex.npy')\n",
    "    print(\"Text...\")\n",
    "    db           = pd.read_csv('../../../data/DATA_LAUDOS_TEXTO_formato1', sep = \";\")\n",
    "    print(\"Labels...\")\n",
    "    resultLabels = np.load('../../../data/resultados/scores/allLabels.npy')\n",
    "    print(\"Data loaded!\\n\")\n",
    "    \n",
    "    print(\"Preprocessing data:\\n\")\n",
    "    print(\"Selecting slices...\")\n",
    "    text_data      = db[db.index.isin(greenZoneIdx)]\n",
    "    labels   = [resultLabels[i] for i in greenZoneIdx]\n",
    "    # dados_validacao = dados_texto[dados_texto['ID_EXAME'].isin(ids_achados['id'][-400000:])]\n",
    "\n",
    "    text   = text_data['CONTEUDO']\n",
    "    text   = text[:-300000]\n",
    "    labels = labels[:-300000]\n",
    "    \n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, fit = False):\n",
    "    print(\"Tokenizing...\")\n",
    "    # Creating vocabulary\n",
    "    if fit:\n",
    "        tokenizer.fit_on_texts(text)\n",
    "    # Vectorizing text\n",
    "    train_X   = tokenizer.texts_to_sequences(text)\n",
    "    # Saving tokenizer\n",
    "    with open(\"../../light_data/pytorch_tokenizer_\"+TOP_WORDS+\".pickle\", 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_DATA:\n",
    "    text, labels = load_data()\n",
    "    text         = [clean_text(i) for i in text]\n",
    "    tokenizer = Tokenizer(num_words = TOP_WORDS, split = ' ')\n",
    "    train_X   = tokenize(text, tokenizer, fit = True)\n",
    "    np.save(\"../../../data/training_data/training_data_X_\"+TOP_WORDS+\".npy\", train_X)\n",
    "else:\n",
    "    labels       = np.load('../../../data/resultados/scores/allLabels.npy')\n",
    "    greenZoneIdx = np.load('../../light_data/greenZoneIndex.npy')\n",
    "    labels       = [labels[i] for i in greenZoneIdx]\n",
    "    train_X      = np.load('../../../data/training_data/training_data_X_'+TOP_WORDS+'.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "torch.save(net.state_dict(), \"../../../data/trained_models/pytorch_checkpoint_3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming lists into tensors...\n",
      "Data preprocessed!\n",
      "\n",
      "Number of input dimensions:  423\n"
     ]
    }
   ],
   "source": [
    "train_y  = labels\n",
    "print(\"Transforming lists into tensors...\")\n",
    "train_X  = [torch.Tensor(i).type(torch.LongTensor) for i in train_X]\n",
    "train_X  = pad_sequence(train_X, batch_first=True).type(torch.LongTensor)\n",
    "train_y  = torch.Tensor(train_y)\n",
    "seq_size = len(train_X[0])\n",
    "print(\"Data preprocessed!\\n\")\n",
    "print(\"Number of input dimensions: \", seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NN...\n",
      "\n",
      "Alexa, play eye of the tiger. It's train time!\n",
      "\n",
      "\n",
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.70it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  0 !\n",
      " Loss:  tensor(0.0547, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.73it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  1 !\n",
      " Loss:  tensor(0.0200, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.70it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  2 !\n",
      " Loss:  tensor(0.0183, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.72it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  3 !\n",
      " Loss:  tensor(0.0305, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.68it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  4 !\n",
      " Loss:  tensor(0.0345, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.68it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  5 !\n",
      " Loss:  tensor(0.0303, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.79it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  6 !\n",
      " Loss:  tensor(0.0300, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.67it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  7 !\n",
      " Loss:  tensor(0.0290, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.66it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  8 !\n",
      " Loss:  tensor(0.0270, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.67it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  9 !\n",
      " Loss:  tensor(0.0269, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  10 !\n",
      " Loss:  tensor(0.0235, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.68it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  11 !\n",
      " Loss:  tensor(0.0216, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.67it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  12 !\n",
      " Loss:  tensor(0.0233, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.66it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  13 !\n",
      " Loss:  tensor(0.0282, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.68it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  14 !\n",
      " Loss:  tensor(0.0214, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:05<00:00,  6.67it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  15 !\n",
      " Loss:  tensor(0.0219, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  16 !\n",
      " Loss:  tensor(0.0231, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  17 !\n",
      " Loss:  tensor(0.0294, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.64it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  18 !\n",
      " Loss:  tensor(0.0179, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.62it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  19 !\n",
      " Loss:  tensor(0.0190, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.62it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  20 !\n",
      " Loss:  tensor(0.0192, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  21 !\n",
      " Loss:  tensor(0.0185, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.66it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  22 !\n",
      " Loss:  tensor(0.0222, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.66it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  23 !\n",
      " Loss:  tensor(0.0164, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.66it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  24 !\n",
      " Loss:  tensor(0.0189, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.64it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  25 !\n",
      " Loss:  tensor(0.0192, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  26 !\n",
      " Loss:  tensor(0.0172, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.65it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:06,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  27 !\n",
      " Loss:  tensor(0.0164, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.64it/s]\n",
      "  2%|▎         | 1/40 [00:00<00:05,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  28 !\n",
      " Loss:  tensor(0.0231, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Epoch  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch  29 !\n",
      " Loss:  tensor(0.0226, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>) \n",
      "\n",
      "Finished training! Loss:  tensor(0.0226, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/trained_models/pytorch_checkpoint_3.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-fd5dea95cb66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training! Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSaving model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../../data/trained_models/pytorch_checkpoint_3.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/trained_models/pytorch_checkpoint_3.pth'"
     ]
    }
   ],
   "source": [
    "net = Net(seq_size).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0005)\n",
    "loss_function = nn.BCELoss()\n",
    "# Training the model\n",
    "print(\"\\nAlexa, play eye of the tiger. It's train time!\\n\\n\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "#         print(i, i+BATCH_SIZE)\n",
    "        batch_X = train_X[i:i+BATCH_SIZE]\n",
    "        batch_y = train_y.squeeze()[i:i+BATCH_SIZE]\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss    = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"End of Epoch \", epoch, \"!\\n Loss: \", loss,\"\\n\")\n",
    "\n",
    "print(\"Finished training! Loss: \", loss)\n",
    "print(\"\\nSaving model...\")\n",
    "torch.save(net.state_dict(), \"../../../data/trained_models/pytorch_checkpoint_3.pth\")\n",
    "print(\"Finish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
